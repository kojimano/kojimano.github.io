<!DOCTYPE HTML>
<html lang="en">
  
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link href="stylesheet.css" rel="stylesheet" type="text/css">
  <link rel="icon" type="image/x-icon" href="images/cornell_icon.png">
  <title>Noriyuki Kojima・コジマ ノリユキ・小島　熙之</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Noriyuki Kojima</name>
              </p>
              <p> I am a fourth-year CS Ph.D student at Cornell University, working with <a href="https://yoavartzi.com/">Yoav Artzi</a>. I am located at <a href="https://www.tech.cornell.edu/">Cornell Tech</a>. I am grateful that My Ph.D. study is supported by Cornell Hopcroft Fellowship (2019-2020) and <a href="https://masason-foundation.org/en/">Masason Fellowship</a> (2020~2022). 
              </p>
              <p>
                Previously, I was a Research Scientist Intern at Hugging Face in summer 2022, working with <a href="https://rush-nlp.com/h">Sasha Rush</a>. and <a href="https://huggingface.co/VictorSanh">Victor Sanh</a>;                
                I was a Research Specialist at Princeton University in 2019, working with <a href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>; I did my undergraduate at the University of Michigan from 2015 to 2018 with B.S.E in Computer Science where I worked with <a href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>, <a href="https://web.eecs.umich.edu/~mihalcea/">Rada Mihalcea</a> and <a href="https://cpsc.yale.edu/people/dragomir-radev">Dragomir Radev</a>; I was a Software Engineer Intern in the Facebook Applied Machine Learning team in summer 2017, where I was supervised by <a href="https://research.fb.com/people/pino-juan/">Juan Pino</a>.
              </p>
              <p style="text-align:center">
                <a href="nk654@cornell.edu">Email</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp-->
                <!-- <a href="data/JonBarron-bio.txt">Biography</a> &nbsp/&nbsp-->
                <a href="https://scholar.google.com/citations?user=Dgu63dgAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/kojimano/"> LinkedIn </a>  &nbsp/&nbsp
                <a href="https://twitter.com/noriyuki_kojima"> Twitter </a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/noriyuki-kojima-8-4.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/noriyuki-kojima-8-4.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in natural language processing, computer vision, and machine learning. Much of my research is about how to learn natural language generation or understanding systems (a) through interactions with humans (b) while perceiving rich visuals involved. Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- Markup-to-Image Diffusion Models with Scheduled Sampling -->
          <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mipnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/molecules_rendering.gif" type="video/mp4" style=padding-top:20px;>
                </video></div>
                <img src='images/molecules_rendering.gif' width="160" style=padding-top:20px;>
              </div>
              <script type="text/javascript">
                function mipnerf_start() {
                  document.getElementById('mipnerf_image').style.opacity = "1";
                }
                function mipnerf_stop() {
                  document.getElementById('mipnerf_image').style.opacity = "0";
                }
                mipnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2210.05147">
                <papertitle>Markup-to-Image Diffusion Models with Scheduled Sampling</papertitle>
              </a>
              <br>
              <a href=https://yuntiandeng.com//>Yuntian Deng</a>,
              <strong>Noriyuki Kojima</strong>,
              <a href="http://rush-nlp.com/">Alexander Rush</a>,
              <br>
              <br>
              <a href="https://huggingface.co/spaces/yuntian-deng/latex2im">demo</a>
              /
              <a href="https://arxiv.org/abs/2210.05147">arXiv</a>
              <p> Studying diffusion models for markup-to-image generation.</p>
            </td>
          </tr> 
          
          <!-- Continual Learning for Grounded Instruction Generation by Observing Human Following Behavior -->
          <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mipnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="videos/tacl2021.mp4" type="video/mp4">
                </video></div>
                <img src='images/tacl2021.jpg' width="160" style=padding-top:32px;>
              </div>
              <script type="text/javascript">
                function mipnerf_start() {
                  document.getElementById('mipnerf_image').style.opacity = "1";
                }
                function mipnerf_stop() {
                  document.getElementById('mipnerf_image').style.opacity = "0";
                }
                mipnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Continual Learning for Grounded Instruction Generation by Observing Human Following Behavior</papertitle>
              <br>
              <strong>Noriyuki Kojima</strong>,
              <a href="http://alanesuhr.com/">Alane Suhr</a>,
              <a href="https://yoavartzi.com/">Yoav Artzi</a>
              <br>
							<em>TACL (presented at EMNLP)</em>, 2021 
              <br>
              <a href="https://lil.nlp.cornell.edu/cerealbar/">project page</a>
              /
              <a href="https://arxiv.org/abs/2108.04812">arXiv</a>
              /
              <a href="https://www.youtube.com/watch?v=KkgIMPTS7H0&t=1s">video</a>
							/
              <a href="https://github.com/lil-lab/cerealbar_generation">code</a>
              <p></p>
              <p>Studying continual learning for natural language instruction generation by observing human users' instruction execution in a collaborative game.</p>
            </td>
          </tr> 

          <!-- What is Learned in Visually Grounded Neural Syntax Acquisition -->
          <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mipnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/acl2020.gif" type="video/mp4" style=padding-top:20px;>
                </video></div>
                <img src='images/acl2020.gif' width="160" style=padding-top:20px;>
              </div>
              <script type="text/javascript">
                function mipnerf_start() {
                  document.getElementById('mipnerf_image').style.opacity = "1";
                }
                function mipnerf_stop() {
                  document.getElementById('mipnerf_image').style.opacity = "0";
                }
                mipnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2005.01678">
                <papertitle>What is Learned in Visually Grounded Neural Syntax Acquisition</papertitle>
              </a>
              <br>
              <strong>Noriyuki Kojima</strong>,
              <a href=http://www.cs.cornell.edu/~hadarelor/>Hadar Averbuch-Elor</a>,
              <a href="http://rush-nlp.com/">Alexander Rush</a>,
              <a href="https://yoavartzi.com/">Yoav Artzi</a>
              <br>
              <em>ACL (Short Paper)</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2005.01678">arXiv</a>
              /
              <a href="https://www.dropbox.com/s/dx1ecbvdsyvd0cl/Presentation.mov?dl=0">video</a>
              /
              <a href="https://github.com/lil-lab/vgnsl_analysis">code</a>
              <p></p>
              <p>Exploring what is learned by the unsupervised vision and language constituency parser via extreme simplification of model architectures.</p>
            </td>
          </tr> 

          <!-- OASIS: A Large-Scale Dataset for Single Image 3D in the Wild -->
          <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mipnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="videos/oasis.gif" type="video/mp4" style=padding-top:20px;>
                </video></div>
                <img src='images/oasis.gif' width="160" style=padding-top:20px;>
              </div>
              <script type="text/javascript">
                function mipnerf_start() {
                  document.getElementById('mipnerf_image').style.opacity = "1";
                }
                function mipnerf_stop() {
                  document.getElementById('mipnerf_image').style.opacity = "0";
                }
                mipnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2007.13215">
                <papertitle>OASIS: A Large-Scale Dataset for Single Image 3D in the Wild</papertitle>
              </a>
              <br>
              <a href="http://www-personal.umich.edu/~wfchen/">Weifeng Chen</a>,
              <a href="https://jasonqsy.github.io/">Shengyi Qian</a>,
              <a href="https://davidfan.io/">David Fan</a>,
              <strong>Noriyuki Kojima</strong>,
              Max Hamilton,
              <a href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br>
              <em>CVPR</em>, 2020
              <br>
              <a href="https://oasis.cs.princeton.edu/">project page</a>
              /
              <a href="https://arxiv.org/abs/2007.13215">arXiv</a>
              /
              <a href="https://oasis.cs.princeton.edu/download">data</a>
              /
              <a href="https://github.com/princeton-vl/oasis">code</a>
              <p></p>
              <p>Presenting Open Annotations of Single Image Surfaces (OASIS): a dataset for images in the wild with dense annotations of detailed 3D geometry.</p>
            </td>
          </tr> 

          <!-- Representing Movie Characters in Dialogues -->
          <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mipnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/conll19.jpg" type="video/mp4" style=padding-top:20px;>
                </video></div>
                <img src='images/conll19.jpg' width="160" style=padding-top:20px;>
              </div>
              <script type="text/javascript">
                function mipnerf_start() {
                  document.getElementById('mipnerf_image').style.opacity = "1";
                }
                function mipnerf_stop() {
                  document.getElementById('mipnerf_image').style.opacity = "0";
                }
                mipnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://aclanthology.org/K19-1010.pdf">
                <papertitle>Representing Movie Characters in Dialogues</papertitle>
              </a>
              <br>
              <a href="http://web.eecs.umich.edu/~mazab/">Mahmoud Azab</a>,
              <strong>Noriyuki Kojima</strong>,
              <a href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>,
              <a href="https://web.eecs.umich.edu/~mihalcea/">Rada Mihalcea</a>
              <br>
              <em>CoNLL</em>, 2019 
              <br>
              <a href="https://aclanthology.org/K19-1010.pdf">paper</a>
              /
              <a href="http://web.eecs.umich.edu/~mihalcea/downloads/CharacterRelatedness.zip">data</a>
              <p></p>
              <p>Representing character names by adding social networks of discourse in embedding objectives.</p>
            </td>
          </tr> 

          <!-- To Learn or Not to Learn: Analyzing the Role of Learning for Navigation in Virtual Environments -->
          <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mipnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="videos/arxiv20.mp4" type="video/mp4">
                </video></div>
                <img src='images/mipnerf_ipe_yellow.png' width="160">
              </div>
              <script type="text/javascript">
                function mipnerf_start() {
                  document.getElementById('mipnerf_image').style.opacity = "1";
                }
                function mipnerf_stop() {
                  document.getElementById('mipnerf_image').style.opacity = "0";
                }
                mipnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1907.11770">
                <papertitle>To Learn or Not to Learn: Analyzing the Role of Learning for Navigation in Virtual Environments</papertitle>
              </a>
              <br>
              <strong>Noriyuki Kojima</strong>,
              <a href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br>
              <em>Preprint</em>, 2019
              <br>
              <a href="https://arxiv.org/abs/1907.11770">arXiv</a>
              <p></p>
              <p> Comparing learning-based methods and classical methods for navigation in virtual environments. Classical methods > learning-based methods in high-level, but learning-based methods learned useful regularities. </p>
            </td>
          </tr> 

          <!-- Speaker naming in movies -->
          <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mipnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/naacl18.jpg" type="video/mp4" style=padding-left:25px;>
                </video></div>
                <img src='images/naacl18.jpg' height="140" style=padding-left:25px;>
              </div>
              <script type="text/javascript">
                function mipnerf_start() {
                  document.getElementById('mipnerf_image').style.opacity = "1";
                }
                function mipnerf_stop() {
                  document.getElementById('mipnerf_image').style.opacity = "0";
                }
                mipnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1809.08761">
                <papertitle>Speaker naming in movies</papertitle>
              </a>
              <br>
              <a href="http://web.eecs.umich.edu/~mazab/">Mahmoud Azab</a>,
              Mingzhe Wang,
              <a href="http://maxosmith.com/">Max Smith</a>,
              <strong>Noriyuki Kojima</strong>,
              <a href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>,
              <a href="https://web.eecs.umich.edu/~mihalcea/">Rada Mihalcea</a>
              <br>
              <em>NAACL</em>, 2018
              <br>
              <a href="https://arxiv.org/abs/1809.08761">arXiv</a>
              /
              <a href="https://lit.eecs.umich.edu/posters/Azab_poster.pdf">poster</a>
              /
              <a href="https://lit.eecs.umich.edu/files/downloads/speakerNaming.zip">data</a>
              <p></p>
              <p>  Proposing a new model for speaker naming in movies that leverages visual, textual, and acoustic modalities in an unified optimization framework. </p>
            </td>
          </tr> 

          <!-- Structured matching for phrase localization -->
          <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mipnerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/eccv16.jpg" type="video/mp4">
                </video></div>
                <img src='images/eccv16.jpg' height="140">
              </div>
              <script type="text/javascript">
                function mipnerf_start() {
                  document.getElementById('mipnerf_image').style.opacity = "1";
                }
                function mipnerf_stop() {
                  document.getElementById('mipnerf_image').style.opacity = "0";
                }
                mipnerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/chapter/10.1007/978-3-319-46484-8_42">
                <papertitle>Structured matching for phrase localization</papertitle>
              </a>
              <br>
              Mingzhe Wang,
              <a href="http://web.eecs.umich.edu/~mazab/">Mahmoud Azab</a>,
              <strong>Noriyuki Kojima</strong>,
              <a href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>,
              <a href="https://web.eecs.umich.edu/~mihalcea/">Rada Mihalcea</a>
              <br>
              <em>ECCV</em>, 2016
              <br>
              <a href="https://link.springer.com/chapter/10.1007/978-3-319-46484-8_42">paper</a>
              /
              <a href="http://www.eccv2016.org/files/posters/P-3C-40.pdf">poster</a>
              /
              <a href="https://github.com/princeton-vl/structured-matching">code</a>
              <p></p>
              <p>  Proposing structured matching of phrases and regions that encourages the semantic relations between phrases to agree with the visual relations. </p>
            </td>
          </tr> 
          
          
        </tbody></table>



        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/reviewer.jpg" width="160"></td>
            <td width="75%" valign="center">
              Reviewer:  ACL2020, NAACL 2021, <a href="https://2021.aclweb.org/blog/reviewer-list/">ACL2021</a> <font color="red"><strong>(Outstanding Reviewer)</strong></font>, ACL Rolling Review 2021 - 2022
              <br><br>

              <a href="https://alvr-workshop.github.io/">Program Committee: 2nd Workshop on Advances in Language and Vision Research (ALVR)</a>
              <br><br>
            </td>
          </tr>
      </td>
    </tr>
  </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td>
        <br>
        <p align="right">
          <font size="2">
            Last updated: August 2021 <br>
            The template was borrowed from <a href="https://jonbarron.info/">Jon Barron</a>'s <a href="https://github.com/jonbarron/jonbarron_website">implementaion</a>.
          </font>
        </p>
      </td>
    </tr>
  </table>


</body>

</html>
